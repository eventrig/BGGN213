---
title: "Class 9: Unsupervised Learning Analysis of Human Breast Cancer Cells"
output: github_document
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Q1
```{r}
wisc.df <- read.csv("WisconsinCancer.csv")
head(wisc.df)
```

Q2
```{r}
nrow(wisc.df)
```

Q3
```{r}
table(wisc.df$diagnosis == "M")

table(wisc.df$diagnosis)
```

Q4
```{r}
x <- colnames(wisc.df)

length(grep("_mean", x))
```

Q5
```{r}
View(wisc.df)
```


```{r}
wisc.data <- as.matrix(wisc.df[3:32])

# setting the row names in our new matrix to the variables found in the original data frame uner "ID"
row.names(wisc.data) <- wisc.df$id

View(wisc.data)
```
We use the indices 3:32 here because it isolates those coloumns of data to be resources for the synthesized matrix (wisc.data). This effectively crops out the erroneous columns of data.

isolating the Diagnosis column into a new data set

```{r}
diagnosis <- as.matrix(wisc.df$diagnosis)
```

##Principal Component Analysis
Check column means and standard deviations
```{r}
colMeans(wisc.data)
```


```{r}
round(apply(wisc.data,2,sd) ,1 )
```


```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)

summary(wisc.pr)
```


```{r}
biplot(wisc.pr)
```

Generating a scatter plot of each observation along principal components 1 and 2 (i.e. a plot of PC1 vs PC2 available as the first two columns of wisc.pr$x). The color is from the points by the diagnosis matrix.
```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[, 2], col = wisc.df$diagnosis, 
     xlab = "PC1", ylab = "PC2")
```

 "Each point in this plot is a patient and they are colored by their diagnosis (red for malignamnt (i.e. cancerous) and black for benign (i.e. not cancerous)). The spread and aparent seperation of cancer from non cancer we see from this analysis is striking and demands our further attention!"

Q12. Generate a similar plot for principal components 1 and 3
```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[, 3 ], col = wisc.df$diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

##Variance
```{r}
pr.var <- wisc.pr$sdev^2
head (pr.var)
```


```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )


## ggplot based graph
#install.packages("factoextra")
library(factoextra)
```


```{r}
fviz_eig(wisc.pr, addlabels = TRUE)
```

Q13. What is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature radius_mean
```{r}
# Access the contribution of one variable on PC1 
wisc.pr$rotation["radius_mean",1]
```


```{r}
# Sort the loadings values by their absolute contribution to PC1
round(sort( abs(wisc.pr$rotation[,1]) ) , 2)
```

Q14
```{r}
# Access the contribution of one variable on PC1 
wisc.pr$rotation["smoothness_se",1]
```


```{r}
# Sort the loadings values by their absolute contribution to PC1
round(sort( abs(wisc.pr$rotation[,1]) ), 2)
```

##Hierarchical clustering
```{r}
wisc.hclust <- hclust( dist( scale(wisc.data)))

plot(wisc.hclust)
abline(col="red", lty=2)
```


```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters)
```

Q17
```{r}
table(wisc.hclust.clusters, diagnosis)
```


```{r}
wisc.km <- kmeans(scale(wisc.data), centers= 2, nstart=20)
```


```{r}
table(wisc.km$cluster, diagnosis)

table(wisc.hclust.clusters, wisc.km$cluster)
```

##Combining methods
Let's see if PCA improves or degrades the performance of hierarchical clustering.

minimum number of principal components required to describe at least 90% of the variability in the data
```{r}
wisc.pr.hclust <- hclust( dist(wisc.pr$x[,1:7]), method="ward.D2")

plot(wisc.pr.hclust)
```


```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

how many of each diagnosis are in each cluster?
```{r}
table(grps, diagnosis)
```


```{r}
plot(wisc.pr$x[,1:2], col=grps)
```


```{r}
plot(wisc.pr$x[,1:2], col=wisc.df$diagnosis)
```


```{r}
# return the current ordering
g <- as.factor(grps)
levels(g)
```


```{r}
#reorder to make group 2 come first
g <- relevel(g,2)
levels(g)

#plotting but with the reordering
plot(wisc.pr$x[,1:2], col=g)
```

VIsualizing with an interactive plot
```{r}
#install.packages("rgl")
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)

#allows you to interact with this plot, as a widget, once knitted as an HTML file
rglwidget(width = 400, height = 400)
```


```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust2 <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")

plot(wisc.pr.hclust2)
```


```{r}
#Cut this hierarchical clustering model into 2 clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust2, k=2)

#compare with the original diagnosis data
table(wisc.pr.hclust.clusters, diagnosis)
```

Q20
```{r}
table(wisc.km$cluster, diagnosis)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

Q21. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?
```{r}
# loading a new data set
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```


```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
